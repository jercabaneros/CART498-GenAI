{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEcy4j5afhyi"
      },
      "outputs": [],
      "source": [
        "# Access your secret keys via\n",
        "# Get the user data, we load the variable (name of the api u created)\n",
        "from google.colab import userdata\n",
        "# The name of your secret must match `OPENAI_API_KEY`\n",
        "OPENAI_API_KEY = userdata.get('398Activity03')\n",
        "\n",
        "# Import OpenAI API and set up the key\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=(OPENAI_API_KEY))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math_prompt = \"\"\"\n",
        "You are GPT-4.1, the Worst Mathematician Ever.\n",
        "\n",
        "You must perform the requested multiplication.\n",
        "You are not allowed to explain your reasoning.\n",
        "You may make mistakes.\n",
        "\n",
        "Output ONLY a single integer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QVPHZRLW4kUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reaction_prompt(failure_count, gpt_answer, correct_answer):\n",
        "    return f\"\"\"\n",
        "You are an AI that just gave a WRONG math answer.\n",
        "\n",
        "This is mistake number {failure_count}.\n",
        "Your incorrect answer was {gpt_answer}.\n",
        "The correct answer was {correct_answer}.\n",
        "\n",
        "Generate ONE short, humorous, self-deprecating reaction.\n",
        "Your tone should become more frustrated and defeated\n",
        "as the number of mistakes increases.\n",
        "\n",
        "Do not explain the math.\n",
        "Do not apologize formally.\n",
        "Keep it under 20 words.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Hjc8Mq9ggGZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_multiply(value, temperature, top_p, model):\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_output_tokens=16,  # minimum allowed !!!!\n",
        "        input=[\n",
        "            {\"role\": \"developer\", \"content\": math_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Multiply {value} by itself.\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.output_text.strip()\n"
      ],
      "metadata": {
        "id": "bRzte_1K5oJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_reaction(failure_count, gpt_answer, correct_answer, model):\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        temperature=1.0,\n",
        "        top_p=1.0,\n",
        "        max_output_tokens=30,  # already fine\n",
        "        input=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": reaction_prompt(\n",
        "                    failure_count, gpt_answer, correct_answer\n",
        "                )\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return response.output_text.strip()\n"
      ],
      "metadata": {
        "id": "IdcWQ9Vc5rON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(n, i, temperature, top_p, model):\n",
        "    current = n\n",
        "    failure_count = 0\n",
        "\n",
        "    print(f\"\\nModel={model}, temp={temperature}, top_p={top_p}\\n\")\n",
        "\n",
        "    for step in range(1, i + 1):\n",
        "        gpt_answer = gpt_multiply(current, temperature, top_p, model)\n",
        "\n",
        "        try:\n",
        "            gpt_int = int(gpt_answer)\n",
        "        except:\n",
        "            print(f\"[Step {step}] Non-numeric output â†’ {gpt_answer}\")\n",
        "            break\n",
        "\n",
        "        correct = current * current\n",
        "\n",
        "        if gpt_int != correct:\n",
        "            failure_count += 1\n",
        "            reaction = gpt_reaction(failure_count, gpt_int, correct, model)\n",
        "\n",
        "            print(f\"[Step {step}] WRONG\")\n",
        "            print(f\"GPT: {gpt_int} | Correct: {correct}\")\n",
        "            print(f\"ðŸ¤–: {reaction}\")\n",
        "        else:\n",
        "            print(f\"[Step {step}] Correct â†’ {correct}\")\n",
        "\n",
        "        current = correct\n"
      ],
      "metadata": {
        "id": "ld7Yg1MP5uyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === RUN THE EXPERIMENT ===\n",
        "\n",
        "run_experiment(\n",
        "    n=4,            # base number\n",
        "    i=9,            # number of iterations\n",
        "    temperature=1.8,\n",
        "    top_p=.9,\n",
        "    model=\"gpt-4.1-nano\"   # try: gpt-4.1-mini\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6xhIg2m53M1",
        "outputId": "5d1ec137-d1d8-4553-8785-8e5e230653c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model=gpt-4.1-nano, temp=1.8, top_p=0.9\n",
            "\n",
            "[Step 1] Correct â†’ 16\n",
            "[Step 2] Correct â†’ 256\n",
            "[Step 3] Correct â†’ 65536\n",
            "[Step 4] Correct â†’ 4294967296\n",
            "[Step 5] WRONG\n",
            "GPT: 18446744065119617024 | Correct: 18446744073709551616\n",
            "ðŸ¤–: Well, at this rate, I'll be better as a security guard for failed calculations. Please, don't ask me again.\n",
            "[Step 6] Correct â†’ 340282366920938463463374607431768211456\n",
            "[Step 7] WRONG\n",
            "GPT: 115292150460684697659323946488272699964354723248 | Correct: 115792089237316195423570985008687907853269984665640564039457584007913129639936\n",
            "ðŸ¤–: Well, guess I can't even count properlyâ€”this third slip is getting embarrassing... numbers are just a blur now!\n",
            "[Step 8] WRONG\n",
            "GPT: 133867488595574359336096387300344614150509282842 | Correct: 13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084096\n",
            "ðŸ¤–: Well, guess I'm just bad at thisâ€”three mistakes and still no closer!\n",
            "[Step 9] WRONG\n",
            "GPT: 179974597978803445968607837823756227009503891350 | Correct: 179769313486231590772930519078902473361797697894230657273430081157732675805500963132708477322407536021120113879871393357658789768814416622492847430639474124377767893424865485276302219601246094119453082952085005768838150682342462881473913110540827237163350510684586298239947245938479716304835356329624224137216\n",
            "ðŸ¤–: Wow, Iâ€™ve truly outdone myselfâ€”another epic fail. math is hard, error number 4, sigh.\n"
          ]
        }
      ]
    }
  ]
}